{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Develop linear, ReLU, sigmoid, tanh, and softmax activation functions as a class for neural networks implementation.\n",
    "\n",
    "Develop the class structure and forward propagation including the loss (cost) function implementation for a deep (multilayer) neural network\n",
    "\n",
    "Develop the backpropagation implementation for a deep (multilayer) neural network (This is a mandatory but not graded programming assignment for this time."
   ],
   "id": "4a6f6ac83b481352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T09:31:59.797675Z",
     "start_time": "2025-10-02T09:31:54.947141Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "100acf3ffe801feb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T09:32:43.392660Z",
     "start_time": "2025-10-02T09:32:43.363171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Activation functions\n",
    "class Activation:\n",
    "    def __init__(self, name):\n",
    "        if name == \"linear\":\n",
    "            self.f = self.linear\n",
    "            self.df = self.linear_d\n",
    "        elif name == \"relu\":\n",
    "            self.f = self.relu\n",
    "            self.df = self.relu_d\n",
    "        elif name == \"sigmoid\":\n",
    "            self.f = self.sigmoid\n",
    "            self.df = self.sigmoid_d\n",
    "        elif name == \"tanh\":\n",
    "            self.f = self.tanh\n",
    "            self.df = self.tanh_d\n",
    "        elif name == \"softmax\":\n",
    "            self.f = self.softmax\n",
    "            self.df = self.softmax_d\n",
    "\n",
    "    def linear(self, x): return x\n",
    "    def linear_d(self, x): return np.ones_like(x)\n",
    "\n",
    "    def relu(self, x): return np.maximum(0, x)\n",
    "    def relu_d(self, x): return (x > 0).astype(float)\n",
    "\n",
    "    def sigmoid(self, x): return 1 / (1 + np.exp(-x))\n",
    "    def sigmoid_d(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def tanh(self, x): return np.tanh(x)\n",
    "    def tanh_d(self, x): return 1 - np.tanh(x)**2\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return e / np.sum(e, axis=0, keepdims=True)\n",
    "    def softmax_d(self, x):\n",
    "        s = self.softmax(x)\n",
    "        return s * (1 - s)  # rarely used directly\n",
    "\n",
    "\n",
    "# Deep Neural Network\n",
    "class DNN:\n",
    "    def __init__(self, layers, acts):\n",
    "        # layers: [in, h1, h2, ..., out]\n",
    "        # acts: activation names per layer\n",
    "        self.L = len(layers) - 1\n",
    "        self.W, self.b, self.act = {}, {}, {}\n",
    "        for l in range(1, len(layers)):\n",
    "            self.W[l] = np.random.randn(layers[l], layers[l-1]) * 0.01\n",
    "            self.b[l] = np.zeros((layers[l], 1))\n",
    "            self.act[l] = Activation(acts[l-1])\n",
    "\n",
    "    def forward(self, X):\n",
    "        cache = {\"A0\": X}\n",
    "        A = X\n",
    "        for l in range(1, self.L+1):\n",
    "            Z = self.W[l] @ A + self.b[l]\n",
    "            A = self.act[l].f(Z)\n",
    "            cache[\"Z\"+str(l)] = Z\n",
    "            cache[\"A\"+str(l)] = A\n",
    "        return A, cache\n",
    "\n",
    "    def loss(self, Yhat, Y, kind=\"ce\"):\n",
    "        m = Y.shape[1]\n",
    "        if kind == \"ce\":  # cross entropy\n",
    "            return -np.sum(Y * np.log(Yhat + 1e-8)) / m\n",
    "        elif kind == \"mse\":\n",
    "            return np.mean((Yhat - Y)**2)\n",
    "\n",
    "    def backward(self, Yhat, Y, cache):\n",
    "        grads = {}\n",
    "        m = Y.shape[1]\n",
    "\n",
    "        for l in reversed(range(1, self.L+1)):\n",
    "            A_prev = cache[\"A\"+str(l-1)]\n",
    "            Z = cache[\"Z\"+str(l)]\n",
    "\n",
    "            if l == self.L:  # output layer\n",
    "                dZ = Yhat - Y\n",
    "            else:\n",
    "                dA = grads[\"dA\"+str(l)]\n",
    "                dZ = dA * self.act[l].df(Z)\n",
    "\n",
    "            grads[\"dW\"+str(l)] = (1/m) * dZ @ A_prev.T\n",
    "            grads[\"db\"+str(l)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            grads[\"dA\"+str(l-1)] = self.W[l].T @ dZ\n",
    "        return grads\n",
    "\n",
    "    def update(self, grads, lr=0.01):\n",
    "        for l in range(1, self.L+1):\n",
    "            self.W[l] -= lr * grads[\"dW\"+str(l)]\n",
    "            self.b[l] -= lr * grads[\"db\"+str(l)]\n"
   ],
   "id": "29e5c64f83b0e102",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T09:33:28.448751Z",
     "start_time": "2025-10-02T09:33:28.439678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4 input -> 5 hidden -> 3 output (softmax)\n",
    "net = DNN([4, 5, 3], [\"relu\", \"softmax\"])\n",
    "\n",
    "X = np.random.randn(4, 10)  # 10 samples\n",
    "Y = np.eye(3)[:, np.random.choice(3, 10)]  # one-hot labels\n",
    "\n",
    "Yhat, cache = net.forward(X)\n",
    "print(\"Loss:\", net.loss(Yhat, Y))\n",
    "\n",
    "grads = net.backward(Yhat, Y, cache)\n",
    "net.update(grads, lr=0.1)\n"
   ],
   "id": "c1edeb684bb18349",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0986519104491905\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
