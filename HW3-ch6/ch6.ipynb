{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-25T21:03:46.613039Z",
     "start_time": "2025-09-25T21:03:46.246778Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Activation Functions\n",
    "\n",
    "class Activation:\n",
    "    def __init__(self, func=\"sigmoid\"):\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, z):\n",
    "        if self.func == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.func == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        elif self.func == \"tanh\":\n",
    "            return np.tanh(z)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation\")\n",
    "\n",
    "    def derivative(self, z):\n",
    "        if self.func == \"sigmoid\":\n",
    "            s = self.forward(z)\n",
    "            return s * (1 - s)\n",
    "        elif self.func == \"relu\":\n",
    "            return (z > 0).astype(float)\n",
    "        elif self.func == \"tanh\":\n",
    "            return 1 - np.tanh(z) ** 2\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation\")\n",
    "\n",
    "\n",
    "\n",
    "# Layer\n",
    "class Layer:\n",
    "    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n",
    "        self.W = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        self.activation = Activation(activation)\n",
    "        self.Z = None\n",
    "        self.A = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z = np.dot(X, self.W) + self.b\n",
    "        self.A = self.activation.forward(self.Z)\n",
    "        return self.A\n",
    "\n",
    "\n",
    "\n",
    "# Loss Function\n",
    "class LossFunction:\n",
    "    def __init__(self, loss=\"binary_crossentropy\"):\n",
    "        self.loss = loss\n",
    "\n",
    "    def compute(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        if self.loss == \"binary_crossentropy\":\n",
    "            return - (1/m) * np.sum(y_true*np.log(y_pred+1e-9) + (1-y_true)*np.log(1-y_pred+1e-9))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss\")\n",
    "\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        return y_pred - y_true  # for sigmoid + BCE\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Forward Propagation\n",
    "# -------------------------------\n",
    "class ForwardProp:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def run(self, X):\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "\n",
    "\n",
    "# Backward Propagation\n",
    "class BackProp:\n",
    "    def __init__(self, layers, loss_func):\n",
    "        self.layers = layers\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "    def run(self, X, y):\n",
    "        grads = {}\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Forward values already stored in layers\n",
    "        L = len(self.layers)\n",
    "        y_pred = self.layers[-1].A\n",
    "\n",
    "        # Output layer gradient\n",
    "        dZ = self.loss_func.derivative(y, y_pred)\n",
    "        grads[\"dW\" + str(L)] = (1/m) * np.dot(self.layers[-2].A.T, dZ)\n",
    "        grads[\"db\" + str(L)] = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "\n",
    "        # Backprop through hidden layers\n",
    "        dA = np.dot(dZ, self.layers[-1].W.T)\n",
    "        for l in reversed(range(L-1)):\n",
    "            dZ = dA * self.layers[l].activation.derivative(self.layers[l].Z)\n",
    "            A_prev = X if l == 0 else self.layers[l-1].A\n",
    "            grads[\"dW\" + str(l+1)] = (1/m) * np.dot(A_prev.T, dZ)\n",
    "            grads[\"db\" + str(l+1)] = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "            if l > 0:\n",
    "                dA = np.dot(dZ, self.layers[l].W.T)\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "# Optimizer: Gradient Descent\n",
    "class GradDescent:\n",
    "    def __init__(self, layers, learning_rate=0.01):\n",
    "        self.layers = layers\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, grads):\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            layer.W -= self.lr * grads[\"dW\" + str(idx+1)]\n",
    "            layer.b -= self.lr * grads[\"db\" + str(idx+1)]\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "class Training:\n",
    "    def __init__(self, model, loss_func, backprop, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.backprop = backprop\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def fit(self, X, y, epochs=1000):\n",
    "        for i in range(epochs):\n",
    "            # Forward\n",
    "            y_pred = self.model.forward.run(X)\n",
    "\n",
    "            # Loss\n",
    "            loss = self.loss_func.compute(y, y_pred)\n",
    "\n",
    "            # Backward\n",
    "            grads = self.backprop.run(X, y)\n",
    "\n",
    "            # Update\n",
    "            self.optimizer.step(grads)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, layer_dims, activations):\n",
    "        \"\"\"\n",
    "        layer_dims: [input_dim, h1, h2, ..., output_dim]\n",
    "        activations: list of activation functions for each layer (len = L-1)\n",
    "        \"\"\"\n",
    "        assert len(layer_dims) - 1 == len(activations)\n",
    "        self.layers = []\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.layers.append(Layer(layer_dims[i-1], layer_dims[i], activation=activations[i-1]))\n",
    "        self.forward = ForwardProp(self.layers)\n",
    "\n",
    "\n",
    "\n",
    "# Example (DNN with 2 hidden layers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # XOR dataset\n",
    "    X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "    y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "    # Build model: input=2, hidden1=4, hidden2=3, output=1\n",
    "    model = Model(layer_dims=[2, 4, 3, 1], activations=[\"relu\", \"relu\", \"sigmoid\"])\n",
    "    loss_func = LossFunction()\n",
    "    backprop = BackProp(model.layers, loss_func)\n",
    "    optimizer = GradDescent(model.layers, learning_rate=0.1)\n",
    "    trainer = Training(model, loss_func, backprop, optimizer)\n",
    "\n",
    "    trainer.fit(X, y, epochs=1000)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931\n",
      "Epoch 100, Loss: 0.6931\n",
      "Epoch 200, Loss: 0.6931\n",
      "Epoch 300, Loss: 0.6931\n",
      "Epoch 400, Loss: 0.6931\n",
      "Epoch 500, Loss: 0.6931\n",
      "Epoch 600, Loss: 0.6931\n",
      "Epoch 700, Loss: 0.6931\n",
      "Epoch 800, Loss: 0.6931\n",
      "Epoch 900, Loss: 0.6931\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
