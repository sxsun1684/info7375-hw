{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T20:56:45.084961Z",
     "start_time": "2025-09-25T20:56:36.994897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "# Activation Functions\n",
    "class Activation:\n",
    "    def __init__(self, func=\"sigmoid\"):\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, z):\n",
    "        if self.func == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.func == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation\")\n",
    "\n",
    "    def derivative(self, z):\n",
    "        if self.func == \"sigmoid\":\n",
    "            s = self.forward(z)\n",
    "            return s * (1 - s)\n",
    "        elif self.func == \"relu\":\n",
    "            return (z > 0).astype(float)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation\")\n",
    "\n",
    "\n",
    "\n",
    "# Neuron\n",
    "class Neuron:\n",
    "    def __init__(self, input_dim, activation=\"sigmoid\"):\n",
    "        self.weights = np.random.randn(input_dim, 1) * 0.01\n",
    "        self.bias = np.zeros((1, 1))\n",
    "        self.activation = Activation(activation)\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z = np.dot(x, self.weights) + self.bias\n",
    "        self.a = self.activation.forward(self.z)\n",
    "        return self.a\n",
    "\n",
    "\n",
    "\n",
    "# Layer\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_dim, output_dim, activation=\"sigmoid\"):\n",
    "        self.W = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        self.activation = Activation(activation)\n",
    "        self.Z = None\n",
    "        self.A = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z = np.dot(X, self.W) + self.b\n",
    "        self.A = self.activation.forward(self.Z)\n",
    "        return self.A\n",
    "\n",
    "\n",
    "\n",
    "# Parameters (wrapper for weights/biases)\n",
    "class Parameters:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers  # store Layer objects\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Loss Function\n",
    "# -------------------------------\n",
    "class LossFunction:\n",
    "    def __init__(self, loss=\"binary_crossentropy\"):\n",
    "        self.loss = loss\n",
    "\n",
    "    def compute(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        if self.loss == \"binary_crossentropy\":\n",
    "            return - (1/m) * np.sum(y_true*np.log(y_pred+1e-9) + (1-y_true)*np.log(1-y_pred+1e-9))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss\")\n",
    "\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        return -(y_true / (y_pred+1e-9)) + ((1-y_true)/(1-y_pred+1e-9))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Forward Propagation\n",
    "# -------------------------------\n",
    "class ForwardProp:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def run(self, X):\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "\n",
    "\n",
    "\n",
    "# Backward Propagation\n",
    "class BackProp:\n",
    "    def __init__(self, layers, loss_func):\n",
    "        self.layers = layers\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "    def run(self, X, y):\n",
    "        grads = {}\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Output layer gradient\n",
    "        L = len(self.layers)\n",
    "        y_pred = self.layers[-1].A\n",
    "        dZ = y_pred - y  # derivative for sigmoid + cross-entropy\n",
    "        grads[\"dW\" + str(L)] = (1/m) * np.dot(self.layers[-2].A.T, dZ)\n",
    "        grads[\"db\" + str(L)] = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "\n",
    "        # Hidden layer\n",
    "        dA_prev = np.dot(dZ, self.layers[-1].W.T)\n",
    "        dZ = dA_prev * self.layers[-2].activation.derivative(self.layers[-2].Z)\n",
    "        grads[\"dW1\"] = (1/m) * np.dot(X.T, dZ)\n",
    "        grads[\"db1\"] = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "# Gradient Descent Optimizer\n",
    "class GradDescent:\n",
    "    def __init__(self, layers, learning_rate=0.01):\n",
    "        self.layers = layers\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, grads):\n",
    "        # Update hidden layer\n",
    "        self.layers[0].W -= self.lr * grads[\"dW1\"]\n",
    "        self.layers[0].b -= self.lr * grads[\"db1\"]\n",
    "\n",
    "        # Update output layer\n",
    "        self.layers[1].W -= self.lr * grads[\"dW2\"]\n",
    "        self.layers[1].b -= self.lr * grads[\"db2\"]\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "class Training:\n",
    "    def __init__(self, model, loss_func, backprop, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.backprop = backprop\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def fit(self, X, y, epochs=1000):\n",
    "        for i in range(epochs):\n",
    "            # Forward\n",
    "            y_pred = self.model.forward.run(X)\n",
    "\n",
    "            # Loss\n",
    "            loss = self.loss_func.compute(y, y_pred)\n",
    "\n",
    "            # Backward\n",
    "            grads = self.backprop.run(X, y)\n",
    "\n",
    "            # Update\n",
    "            self.optimizer.step(grads)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "# Model\n",
    "class Model:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        # Two layers: hidden + output\n",
    "        hidden = Layer(input_dim, hidden_dim, activation=\"sigmoid\")\n",
    "        output = Layer(hidden_dim, output_dim, activation=\"sigmoid\")\n",
    "        self.layers = [hidden, output]\n",
    "        self.forward = ForwardProp(self.layers)\n",
    "\n",
    "\n",
    "# Example Run\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy dataset (XOR-ish)\n",
    "    X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "    y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "    model = Model(input_dim=2, hidden_dim=2, output_dim=1)\n",
    "    loss_func = LossFunction()\n",
    "    backprop = BackProp(model.layers, loss_func)\n",
    "    optimizer = GradDescent(model.layers, learning_rate=0.1)\n",
    "    trainer = Training(model, loss_func, backprop, optimizer)\n",
    "\n",
    "    trainer.fit(X, y, epochs=1000)\n"
   ],
   "id": "3603a608ea6c8083",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6932\n",
      "Epoch 100, Loss: 0.6931\n",
      "Epoch 200, Loss: 0.6931\n",
      "Epoch 300, Loss: 0.6931\n",
      "Epoch 400, Loss: 0.6931\n",
      "Epoch 500, Loss: 0.6931\n",
      "Epoch 600, Loss: 0.6931\n",
      "Epoch 700, Loss: 0.6931\n",
      "Epoch 800, Loss: 0.6931\n",
      "Epoch 900, Loss: 0.6931\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
